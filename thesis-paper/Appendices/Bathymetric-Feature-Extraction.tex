\chapter{Bathymetric Feature Extraction}
\lhead{Bathymetric Feature Extraction}
\label{Appendix:BathymetricFeatureExtraction}

	The following discussion details the bathymetric feature extraction process. Given the bathymetric depth observations, the other bathymetric features, aspect and rugosity, can be computed in the short scale and long scale form in order to obtain a rich feature space for benthic habitat mapping.
	
	\section{Formulation}
	\label{Appendix:BathymetricFeatureExtraction:Formulation}
	
		With the basic structure of the benthic habitat mapping process in Section \ref{BenthicHabitatMapping:BathymetricFeatures}, the model spaces involved can be formulated as below.
		
		As established previously, the spatial space $\mathcal{P} \subseteq \mathbb{R}^{2}$ and feature space $\mathcal{X} \subseteq \mathbb{R}^{5}$ are compact subsets of $\mathbb{R}^{2}$ and $\mathbb{R}^{5}$ respectively. On the other hand, the habitat map space $\mathcal{Y} \subset \mathbb{N}$ is a strict finite subset of the the natural numbers, with each distinct integer representing a particular habitat type. Further, define $\mathscr{P} := \mathscr{P}(\mathcal{P})$, $\mathscr{X} := \mathscr{X}(\mathcal{X})$, and $\mathscr{Y} := \mathscr{Y}(\mathcal{Y})$ as the $\sigma$-algebras generated by $\mathcal{P} \subseteq \mathbb{R}^{2}$, $\mathcal{X} \subseteq \mathbb{R}^{5}$, and $\mathcal{Y} \subset \mathbb{N}$ respectively. Specifically, this means that $X \in \mathscr{X}$ implies $X \subseteq \mathcal{X}$, and similarly for $\mathscr{P}$ and $\mathscr{Y}$. Note that if $X := \{\bvec{x}_{i}\}_{i \in I_{n}} := \{\bvec{x}_{1}, \bvec{x}_{2}, \dots, \bvec{x}_{n}\} \in \mathscr{X}$ is a \textit{finite} collection of feature vectors $\bvec{x}_{i} \in \mathcal{X}$ for all $i \in I_{n}$, then $X$ is equivalent to the matrix $[\bvec{x}_{1}, \bvec{x}_{2}, \dots, \bvec{x}_{n}]^{T} \in \mathbb{R}^{n \times p}$. This notation is used consistently throughout, and a similar formulation applies to spatial coordinates $P \in \mathscr{P}$ and target labels $\bvec{y} := \{y_{i}\}_{i \in I_{n}} := \{y_{1}, y_{2}, \dots, y_{n}\} \in \mathscr{Y}$. Recall that for an arbitrary quantity $q_{i}$ indexed by $i \in I_{n}$, $\{q_{i}\}_{i \in I_{n}}$ is defined to be $\{q_{1}, q_{2}, \dots, q_{n}\}$ which is equivalent to a vector if $q_{i}$ is a scalar and a matrix if $q_{i}$ is a vector.
			
	\section{Data Matching on Real Datasets}
	\label{Appendix:BathymetricFeatureExtraction:DataMatching}
	
		A subtlety that arises from the above formulation is that during the training stage, the bathymetric data and the label data are not necessarily observed at the same places. Figure \ref{Figure:ScottReefBathymetricFeatures} illustrates the spatial distribution of the two datasets in a typical setting using the Scott Reef data set \citep{IMOS} as an example. Note that the colour scales have been shifted to emphasize the feature variations.
		
		Like the Scott Reef dataset, in most cases the bathymetric observations span the entire region of interest densely. However, while bathymetric data are usually collected rather uniformly and densely, the label data are collected from past AUV missions whose trajectory are continuous paths across a very limited region of the ocean floor \citep{Squidle}. As such, the label data are also spatially concentrated on the mission trajectory, while being almost non-existent elsewhere, making it a multi-modal distributed dataset. However, in order for the GP classifier to learn the relationship between the bathymetric features and habitat labels, the bathymetric and label data must have a one-to-one correspondence. That is, they must exist at the same locations. 
		
		Formally, let $P_{b}, P_{h} \in \mathscr{P}$ be the spatial positions at which bathymetric and label data have been observed. Let $X_{b} \in \mathscr{X}$ be the bathymetric features observed at $P_{b}$ and $\bvec{y}_{h} \in \mathscr{Y}$ be the habitat labels observed at $P_{h}$. The data matching problem is to obtain or infer $X_{h} \in \mathscr{X}$, the bathymetric features at locations $P_{h}$, from $X_{b}$. This is a standard supervised learning problem. Specifically, the aim is to learn an inference model $\zeta: P_{h} \mapsto X_{h}$ from the empirical relationship $P_{b} \mapsto X_{b}$.
		
		While it is possible to employ a separate GP regressor to perform the above inference, this adds unnecessary computational complexity. Since GPs are $O(n^{3})$ algorithms, using a layered GP approach where both stages of the benthic habitat mapping process employs GPs will increase the inference bottleneck. Instead, as there exists an abundance of bathymetric data, without much loss of accuracy it suffices to employ a nearest neighbour interpolation to obtain $X_{h}$. 
			
	\section{Aspect and Rugosity}
	\label{Appendix:BathymetricFeatureExtraction:DataMatching:AspectAndRugosity}
	
		The feature extraction process assumes that the bathymetric depth data is available in grid form. That is, one can represent the available depth data $Z = \{z_{k}\}_{k \in {1, 2, ..., N}}$ as $Z = \{z_{ij}\}_{i \in {1, 2, ..., n_{i}}, \;\; j \in {1, 2, ..., n_{j}}}$ where varying $i$ and $j$ corresponds to varying data points in axis 1 and 2 respectively. Axis 1 and 2 is required to form an orthonormal frame. While axis 1 and 2 is usually aligned with the eastings-northings frame, it is generally not required for the feature extraction process.
		
		Without loss of generality, let $x$ and $y$ denote quantities corresponding to the orthogonal axes. We have that at $(x_{i}, y_{j})$ $(i \in {1, 2, ..., n_{i}}, \;\; j \in {1, 2, ..., n_{j}})$ the depth is measured as $z_{ij}$. The partial derivatives of various degrees of accuracy and scale can then be estimated through central differencing. Table \ref{Table:AspectExtraction}  shows the case for extracting aspect in the $x$-direction. Correspondingly, replacing the operations from $i$ to $j$ the aspect in the $y$-direction can also be similarly extracted.
		
		\bgroup
		\def\arraystretch{2}%  1 is the default, change whatever you need
		\begin{table}[h]
			\begin{center}
				\begin{tabular}{ c c }
					\hline
					\hline
					N & Aspect (Slope) Extraction [$x$-direction]\\
					\hline
					\hline
					3 & $^{3}_{x}a_{i, j} := \frac{- z_{i - 1, j} + z_{i + 1, j}}{2h}$ \\
					5 & $^{5}_{x}a_{i, j} := \frac{z_{i - 2, j} - 8 z_{i - 1, j} + 8 z_{i + 1, j} - z_{i + 2, j}}{12h}$ \\
					7 & $^{7}_{x}a_{i, j} := \frac{-z_{i - 3, j} + 9 z_{i - 2, j} - 45 z_{i - 1, j} + 45 z_{i + 1, j} - 9 z_{i + 2, j} + z_{i + 3, j}}{60h}$ \\
					9 & $^{9}_{x}a_{i, j} := \frac{3 z_{i - 4 j} - 32 z_{i - 3, j} + 168 z_{i - 2, j} - 672 z_{i - 1, j} + 672 z_{i + 1, j} - 168 z_{i + 2, j} + 32 z_{i + 3, j} - 3 z_{i + 4, j}}{840h}$ \\
					\hline
					\hline
				\end{tabular}
			\end{center}
	  	\caption{Aspect feature extraction using finite (central) difference approximations}
	  	\label{Table:AspectExtraction}			
	  	\end{table}	
	 		\egroup
	 		
	 		The chosen spacing employed in this thesis are $N = 3$ neighbors for short scale aspect and $N = 9$ neighbors for large scale aspect. That is, \begin{align*} \numberthis \label{Equation:AspectExtraction}
	 				{_{x}\{a_{s}\}_{i, j}} &:= {^{3}_{x}a_{i, j}} \quad && {_{y}\{a_{s}\}_{i, j}} := {^{3}_{y}a_{i, j}} \quad && {\{a_{s}\}_{i, j}} := \sqrt{{_{x}\{a_{s}\}^{2}_{i, j}} + {_{y}\{a_{s}\}^{2}_{i, j}}} \\
	 				{_{x}\{a_{l}\}_{i, j}} &:= {^{9}_{x}a_{i, j}} \quad && {_{y}\{a_{l}\}_{i, j}} := {^{9}_{y}a_{i, j}} \quad && {\{a_{l}\}_{i, j}} := \sqrt{{_{x}\{a_{l}\}^{2}_{i, j}} + {_{y}\{a_{l}\}^{2}_{i, j}}}
	 		\end{align*}
	 						  					
		Central differencing is chosen as it is more numerically accurate. The disadvantages of instability and slightly higher time complexity from dynamic cases are not present in the static feature extraction process. Nevertheless, forward differencing is to be used at the boundaries of the dataset where neighboring data is missing on one side.
					
		With two axis, the result is a 2 element gradient vector. It is possible to treat the 2 elements as separate features. However, this would make the modeling problem frame dependent which would unnecessarily complicate the modeling process. Therefore, the magnitude of this gradient is taken as the aspect feature \eqref{Equation:AspectExtraction}. 
		
		Rugosity is a measure of local height variations in the terrain. By definition, it is computed with $r = A_{r}/A_{g}$, the real surface area divided by the geometric surface area. Under bathymetry measurements that are geo-referenced through stereo imagery, rugosity can be calculated through a Delaunay triangulated surface mesh and projecting areas onto the plane of best fit using Principal Component Analysis (PCA) \citep{Friedman:Rugosity}.
		
		In the case that the bathymetric data is sufficiently sparse or is not distributed uniformly for grid based methods, then the above feature extraction process becomes inaccurate or even infeasible so that feature extraction itself becomes a prediction problem. When this happens, a Gaussian process regression model is proposed for predicting the features at a given spatial location. While this is much more computationally expensive than performing feature extraction, it is also quite rare that this is necessary under abundant bathymetric data. In most cases, it suffices to simply take the nearest neighbouring feature.
		
		In the benthic habitat mapping scenario, however, part of the reason that bathymetric features are used in the beginning is that bathymetric data is abundant and relatively densely observed. Therefore, it is rare to find a case where the above feature extraction does not apply.
		
		After each feature is extracted, each feature can then be whitened, or standardised, in order to allow better numerical properties for the classifier to model the benthic habitats upon the bathymetric features. While this is not necessarily, it ensures that each feature are roughly in the same numerical range so that no particular feature would dominate the model simply due to differences in the orders of magnitude. To standardise, each training feature set is subtracted by its mean and divided by its standard deviation, which are referred to as the \textit{whiten parameters}. The query feature set then undergoes the same transformation using the whiten parameters from the training set to allow uniform transformations. This covers the whitening process indicated in Figure \ref{Figure:InferenceFlow}.
		