\chapter{Computational Aspects of Gaussian Processes}
\lhead{Computational Aspects of Gaussian Processes}
\label{Appendix:ComputationalAspects}

	While Gaussian processes are extremely flexible models that are able to model a variety of phenomenon, its flexibility also comes with a cost in computational complexity. As such, there are many subtleties involved in its implementation, as well as plenty of methods to improve its performance.
	
	The following sections provides a brief discussion for some of those computational techniques developed in this thesis that can be used for implementing Gaussian processes. While these techniques are initially developed for the work in this thesis, these techniques are general and can be applied to any work that requires the use of Gaussian process models. 
	
	\section{Numerical Stability}
	\label{Appendix:ComputationalAspects:NumericalStability}
	
		Due to limited numerical precisions, numerical computations on the computer may sometimes result in instability of errors even if it is impossible under analytical approaches. Furthermore, as computers cannot generate true random numbers, any sampling technique must employ pseudo random number generators, which could potentially cause problems in the computations if not used carefully. The Monte Carlo prediction information entropy  developed in this thesis requires a sampling step whose numerical stability can be improved by taking into consideration the points mentioned above through a stable and efficient Monte Carlo optimisation method.
		
		Before such a technique is discussed, however, the Cholesky decomposition matrix must be first introduced.
		
		\subsection{Cholesky Decomposition}
		\label{Appendix:ComputationalAspects:NumericalStability:Cholesky}
			
			In linear algebra, the Cholesky decomposition of a positive definite matrix $S$ is a lower triangular matrix $L$ that satisfies \eqref{Equation:CholeskyDecomposition}.
			
			\begin{equation}
				S = L L^{T}
			\label{Equation:CholeskyDecomposition}
			\end{equation}
			
			Every positive definite matrix has a unique Cholesky decomposition. Once a Cholesky decomposition is computed, any inversion problem of the form \eqref{Equation:InversionProblem} can then be solved by \eqref{Equation:InversionProblemCholesky} instead, which are more stable and efficient since $L$ and $L^{T}$ are now triangular matrices.
			
			\begin{equation}
				S \bvec{x} = \vec{b} \implies \bvec{x} = S \backslash \vec{b}
			\label{Equation:InversionProblem}
			\end{equation}
			
			\begin{equation}
				L L^{T} \bvec{x} = \vec{b} \implies \bvec{x} = L^{T} \backslash (L \backslash \vec{b})
			\label{Equation:InversionProblemCholesky}
			\end{equation}
			
		\subsection{Cholesky Jittering}
		\label{Appendix:ComputationalAspects:NumericalStability:CholeskyJittering}

			As mentioned before, even with the more stable and efficient Cholesky decomposition method, there are still cases where a Cholesky decomposition cannot be found as the numerical representation of the positive definite matrix $S$ is slightly off such that it is no longer a positive definite matrix. For example, it may be a positive semi-definite matrix or even slightly negative semi-definite.
			
			As such, a common method to solve this problem is to employ some jittering in the cholesky decomposition process. As a positive definite matrix usually have its most positive values on the diagonals, the method begins by adding small positive numbers, or \textit{jitters}, onto the diagonal. The method then attempts to find the Cholesky decomposition of the new and slightly different matrix. If there is still an error, the process repeats until there is no error in finding the Cholesky decomposition anymore.
			
			While this will not always guarantee a successful Cholesky decomposition, it is used commonly in practice and seldom fails.
			
		\subsection{Stable and Efficient Monte Carlo Optimisation}
		\label{Appendix:ComputationalAspects:NumericalStability:MonteCarlo}
		
			Section \ref{InformativeSeafloorExploration:MCPIE} formulates the Monte Carlo prediction information entropy (MCPIE), which involves a Monte Carlo sampling stage for entropy estimation. In order to enhance the computational tractability of the approach, this thesis also investigates the practical implementation methods which would allow efficient computation for MCPIE.
			
			By definition of a GP, drawing from a GP at a \textit{finite} number of query points $X^{\star}$ is equivalent to drawing \textit{jointly} from a multivariate Gaussian distribution, as represented by \eqref{Equation:BinaryPredictiveGaussianDistribution}, \eqref{Equation:MulticlassPredictiveGaussianDistributionOVA}, and \eqref{Equation:MulticlassPredictiveGaussianDistributionAVA}. In general the sampling stage can be summarised by \eqref{Equation:GeneralMultivariateGaussianDistribution} for some mean vector $\vec{\upmu} \in \mathbb{R}^{n^{\star}}$ and covariance vector $\Sigma \in \mathbb{R}^{n^{\star} \times n^{\star}}$. 
			
			\begin{equation}
				{^{s}}\bvec{f}^{\star} \stackrel{\text{sample}}{\sim} \mathcal{N}(\vec{\upmu}^{\star}, \Sigma^{\star}) \qquad \forall s \in I_{n_{S}}
			\label{Equation:GeneralMultivariateGaussianDistribution}
			\end{equation}
			
			Note that $\stackrel{\text{sample}}{\sim}$ denotes that ${^{s}}\bvec{f}$ is not a random vector anymore but a specific vector after sampling. Instead of drawing jointly from the distribution \eqref{Equation:GeneralMultivariateGaussianDistribution} directly, a more advantageous approach involves first drawing \textit{iid} samples from the standard univariate normal distribution. That is, the samples are to be drawn independently.
			
			\begin{equation}
				\begin{aligned}
					z_{i, s} &\stackrel{\text{sample}}{\sim} \mathcal{N}(0, 1) \qquad \forall i \in I_{n^{\star}}, \forall s \in I_{n_{S}} \\
					\bvec{z}_{s} &:= \{z_{i, s}\}_{i \in I_{n^{\star}}} \in \mathbb{R}^{n^{\star}} \\
					Z &:= \{z_{i, s}\}_{i \in I_{n^{\star}}, \; s \in I_{n_{S}}} \equiv \begin{bmatrix} \bvec{z}_{1} & \bvec{z}_{2} & \dots & \bvec{z}_{n_{S}} \end{bmatrix} \in \mathbb{R}^{n^{\star} \times n_{S}}
				\end{aligned}
			\label{Equation:iidGaussianSampling}
			\end{equation}			
			
			Let $L^{\star}$ be the Cholesky Decomposition of $\Sigma^{\star}$, then $L^{\star} \bvec{z}_{s}$ incorporates the covariance structure \eqref{Equation:CovarianceInclusion}.
			
			\begin{equation}
				\begin{aligned}
					L^{\star} \bvec{z}_{s} \stackrel{\text{sample}}{\sim} \mathcal{N}(\bvec{0}, \Sigma^{\star}) \qquad \forall s \in I_{n_{S}}
				\end{aligned}
			\label{Equation:CovarianceInclusion}
			\end{equation}
			
			To incorporate the expectance $\vec{\upmu}^{\star}$, simply add it to the transformed samples \eqref{Equation:ExpectanceInclusion}.
			
			\begin{equation}
				\begin{aligned}
					\vec{\upmu}^{\star} + L^{\star} \bvec{z}_{s} \stackrel{\text{sample}}{\sim} \mathcal{N}(\vec{\upmu}^{\star}, \Sigma^{\star}) \qquad \forall s \in I_{n_{S}}
				\end{aligned}
			\label{Equation:ExpectanceInclusion}
			\end{equation}
			
			Therefore, to sample from any given multivariate Gaussian distribution $\mathcal{N}(\vec{\upmu}^{\star}, \Sigma^{\star})$, simply draw iid samples through \eqref{Equation:iidGaussianSampling} and transform the samples through \eqref{Equation:ExpectanceInclusion}, and set ${^{s}}\bvec{f}^{\star} := \vec{\upmu}^{\star} + L^{\star} \bvec{z}_{s} \quad \forall s \in I_{n_{S}}$. Generating iid samples from standard normal distributions is much faster, especially with sophisticated libraries which can have pseudo-random number generators specifically for values drawn from the standard normal distribution.
			
			Moreover, to vectorise the computation for further computational efficiency, multiple samples can be obtained at once through \eqref{Equation:VectorisedSampling}.
			
			\begin{equation}
				\begin{aligned}
					\begin{bmatrix} {^{1}}\bvec{f}^{\star} & {^{2}}\bvec{f}^{\star} & \dots & {^{n_{S}}}\bvec{f}^{\star} \end{bmatrix} &= \begin{bmatrix} \vec{\upmu}^{\star} + L^{\star} \bvec{z}_{1} & \vec{\upmu}^{\star} + L^{\star} \bvec{z}_{2} & \dots & \vec{\upmu}^{\star} + L^{\star} \bvec{z}_{n_{S}} \end{bmatrix} \\
					&= \mathcal{U} + L^{\star} Z \qquad \text{where} \qquad \mathcal{U} := \{\vec{\upmu}^{\star}\}^{T}_{s \in I_{n_{S}}}
				\end{aligned}
			\label{Equation:VectorisedSampling}
			\end{equation}			
			
			Under usual circumstances, this transformation technique provides a more time efficient method for drawing samples from a multivariate Gaussian distribution. However, if the samples are to be used for Monte Carlo estimation of a certain quantity, such as the joint PIE, and such a quantity is involved in an optimisation process, the above technique is almost necessary \footnote{There are certainly optimisation algorithms that are designed specifically for such an optimisation process, and are generally found under the technique of \textit{stochastic optimisation}. The transformation technique here however avoids the need for such stochastic methods such that any regular optimiser can be employed.}. In fact, this is precisely the case for MCPIE acquisition, where the joint PIE is estimated through Monte Carlo sampling and is also the objective of the optimisation process (section \ref{InformativeSeafloorExploration:MCPIE}).
			
			The reason is essentially as follows. Since the MCPIE is estimated from samples that are randomly drawn from a probabilistic distribution, the MCPIE itself is random, even if its variance decreases under the limit of large samples. As such, for the same set of query locations, the optimiser may compute slightly different values for the corresponding MCPIE. This makes the optimisation process numerically unstable with poor convergence properties. In fact, under small samples where the variance of MCPIE is large, the optimisation process generally does not converge. This is because variance in the objective or constraint transfers to a variance in the optimiser Lagrangian, which acts as a noise input to the optimiser. Furthermore, most optimisers utilises numerical gradient methods, and derivatives amplifies noise \footnote{With minimum rigour, derivatives are limits of standardised differences whereas integrals are limits of standardised sums. Differences of noisy quantities are relatively more noisy, and sums of noisy quantities are relatively less noisy. As such, derivatives amplifies noise, while integrals smooths away noise. See \cite{Nise:2000:CSE:519085} for more detail.}. As such, the randomness spreads noise throughout the optimiser Lagrangian and corresponding gradients, which destabilises the process as, figuratively speaking, the optimiser is unable discern the peak of the mountain if the terrain is changing like the waves of an ocean. 
			
			The typical way to solve the above problem is to reinitialise the seed to the same value in each optimisation iteration, so that the same samples are drawn for estimation under identical scenarios. Nevertheless, if the transformation technique is used, this is equivalent to drawing a set of iid normal samples $Z$ at the beginning of the optimisation process and continuously reusing these samples for any computation. This avoids resetting the seed and redrawing the samples, thus speeding up the computation. This technique works as, while the distribution to be drawn from in each optimisation iteration changes, such a distribution is entirely captured by the mean vector $\vec{\upmu}^{\star}$ and covariance matrix $\Sigma^{\star}$. Thus, the only quantities that need to change in each iteration are $\vec{\upmu}^{\star}$ and $\Sigma^{\star}$, or equivalently $\mathcal{U}$ and $L^{\star}$ in \eqref{Equation:VectorisedSampling}. That is, through transforming the iid samples with dynamic moments $\vec{\upmu}^{\star}$ and $\Sigma^{\star}$ in every optimisation loop, the optimisation process stabilises and is more efficient than the method of seed reinitialisation.
			
	\section{Time and Spatial Complexity}
	\label{Appendix:ComputationalAspects:TimeComplexity}
	
		As Gaussian processes are typically $O(n^{3})$ algorithms in time and $O(n^{2})$ algorithms in space, there has been a lot of related work in reducing the time and spatial complexity of the algorithms. Some notable examples are sparsifications of covariance kernel matrices and other matrix approximation techniques.
		
		The following sections instead discusses some simple ways to reduce time and spatial complexity without such complicated change to the model implementation.
		
		\subsection{Diagonal Log-Likelihood Hessians}
		\label{Appendix:ComputationalAspects:TimeSpaceComplexity:DiagonalHessians}
					
			For Gaussian process classification under Laplace approximation, the Hessian matrix of the log-likelihood response is a diagonal matrix since each output $y_{i}$ is only correlated to the corresponding latent $f_{i}$ at that point. Therefore, to speed up computations and reduce memory usage, the matrix can be stored in a single axis array which contains its diagonal elements, which would reduce the memory usage from $O(n^{2})$ to $O(n)$.
			
			Furthermore, the Laplace approximation algorithm requires the Cholesky Decomposition of the negative log-likelihood Hessian $W := -\frac{\partial^{2}}{{\partial \bvec{f}}^{2}} \log(p(\bvec{y} | \bvec{f}))$. Since the negative Hessian matrix is diagonal, the Cholesky Decomposition is simply a diagonal matrix whose elements are square roots of the original matrix. This further speeds up computations as square roots can be performed element-by-element wise without the need for a Cholesky decomposition step.
			
		\subsection{Avoiding Full Covariance Computations}
		\label{Appendix:ComputationalAspects:TimeSpaceComplexity:CovarianceAvoidance}
		
			Similar to the case with diagonal log-likelihood Hessians, there are other cases where the full matrix does not need to be computed. 
			
			The computations of the prediction classes and the prediction information entropy are non-mutual. As such, they do not require the covariance structure of the latent Gaussian process. Therefore, in order to compute the prediction map and the entropy maps, one can simply compute the diagonal elements of the covariance matrix, which results in a variance vector. Together with the expectance vector, this provides sufficient information for non mutual analysis.
			
			This reinforces the advantage of using a receding horizon approach for informative path planning. In the receding horizon scheme, the only computations that require a full covariance structure is the computation of the acquisition function along the path. Since the path is composed of much fewer query points than the entire map, it is feasible and achievable to compute such covariances in a reasonable amount of time. Meanwhile, the map itself does not require the covariance structure, and therefore can be quickly computed through a variance vector even though it is composed of many query points.
	
		\subsection{Fast Inference with Learner and Predictor Cache}
		\label{Appendix:ComputationalAspects:TimeSpaceComplexity:Cache}
		
			In many cases, the first approach to reduce time complexity is to avoid recomputing quantities when possible. In the Gaussian process classification scheme under Laplace approximation, the following quantities are shared between the learning and inference stage such that caching such quantities can speed up the computations by a noticeable amount.
			
			\begin{itemize}
				\item Negative Log-Likelihood Hessian Matrix $W := -\frac{\partial^{2}}{{\partial \bvec{f}}^{2}} \log(p(\bvec{y} | \bvec{f}))$
				\item Square Root of Negative Log-Likelihood Hessian Matrix $W^{\frac{1}{2}}$
				\item Log Likelihood Gradient Vector $\frac{\partial}{\partial \bvec{f}} \log(p(\bvec{y} | \bvec{f}))$
				\item Cholesky Decomposition Matrix $L$ of Effective Training Covariance
			\end{itemize}

			By caching these quantities in the learner stage and thus avoiding recomputations in the inference stage, the inference process can be sped up significantly. This summarises the ``Learner Cache'' in Figure \ref{Figure:InferenceFlow}.
			
			Furthermore, as the expectance vector, variance vector, and covariance matrix requires the computation of the inference matrix $K^{\star}$, such a matrix can be cached in the predictor stage, thereby avoiding recomputations further. Together with the already known query features $X^{\star}$, this forms the ``Predictor Cache'' in Figure \ref{Figure:InferenceFlow}.
			
			Together, the ``Learner Cache'' and ``Predictor Cache'' is sufficient to compute the ``Latent Expectance'', ``Latent Covariance'', and ``Latent Variance'', as denoted in Figure \ref{Figure:InferenceFlow}. The latent process is then sufficient to compute all further inference and predictions.
		

		
			
		
		