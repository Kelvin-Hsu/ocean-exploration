\chapter{Computational Aspects of Gaussian Processes}
\lhead{Computational Aspects of Gaussian Processes}
\label{Appendix:ComputationalAspects}

	\section{Numerical Stability}
	\label{Appendix:ComputationalAspects:NumericalStability}
	
		\subsection{Cholesky Decomposition}
		\label{Appendix:ComputationalAspects:NumericalStability:Cholesky}

		\subsection{Cholesky Jittering}
		\label{Appendix:ComputationalAspects:NumericalStability:CholeskyJittering}
				
		\subsection{Solving Triangular Matrix Equations}
		\label{Appendix:ComputationalAspects:NumericalStability:SolvingTriangularMatrices}
		
		\subsection{Stable and Efficient Monte Carlo Optimisation}
		\label{Appendix:ComputationalAspects:NumericalStability:MonteCarlo}
		
			Section \ref{InformativeSeafloorExploration:MCPIE} formulates the Monte Carlo prediction information entropy (MCPIE), which involves a Monte Carlo sampling stage for entropy estimation. In order to enhance the computational tractability of the approach, this thesis also investigates the practical implementation methods which would allow efficient computation for MCPIE.
			
			By definition of a GP, drawing from a GP at a \textit{finite} number of query points $X^{\star}$ is equivalent to drawing \textit{jointly} from a multivariate Gaussian distribution, as represented by \eqref{Equation:BinaryPredictiveGaussianDistribution} and \eqref{Equation:MulticlassPredictiveGaussianDistribution}. In general the sampling stage can be summarised by \eqref{Equation:GeneralMultivariateGaussianDistribution} for some mean vector $\vec{\upmu} \in \mathbb{R}^{n^{\star}}$ and covariance vector $\Sigma \in \mathbb{R}^{n^{\star} \times n^{\star}}$. 
			
			\begin{equation}
				{^{s}}\bvec{f}^{\star} \stackrel{\text{sample}}{\sim} \mathcal{N}(\vec{\upmu}^{\star}, \Sigma^{\star}) \qquad \forall s \in I_{n_{S}}
			\label{Equation:GeneralMultivariateGaussianDistribution}
			\end{equation}
			
			Note that $\stackrel{\text{sample}}{\sim}$ denotes that ${^{s}}\bvec{f}$ is not a random vector anymore but a specific vector after sampling. Instead of drawing jointly from the distribution \eqref{Equation:GeneralMultivariateGaussianDistribution} directly, a more advantageous approach involves first drawing \textit{iid} samples from the standard univariate normal distribution. That is, the samples are to be drawn independently.
			
			\begin{equation}
				\begin{aligned}
					z_{i, s} &\stackrel{\text{sample}}{\sim} \mathcal{N}(0, 1) \qquad \forall i \in I_{n^{\star}}, \forall s \in I_{n_{S}} \\
					\bvec{z}_{s} &:= \{z_{i, s}\}_{i \in I_{n^{\star}}} \in \mathbb{R}^{n^{\star}} \\
					Z &:= \{z_{i, s}\}_{i \in I_{n^{\star}}, \; s \in I_{n_{S}}} \equiv \begin{bmatrix} \bvec{z}_{1} & \bvec{z}_{2} & \dots & \bvec{z}_{n_{S}} \end{bmatrix} \in \mathbb{R}^{n^{\star} \times n_{S}}
				\end{aligned}
			\label{Equation:iidGaussianSampling}
			\end{equation}			
			
			Let $L^{\star}$ be the Cholesky Decomposition of $\Sigma^{\star}$, then $L^{\star} \bvec{z}_{s}$ incorporates the covariance structure \eqref{Equation:CovarianceInclusion}.
			
			\begin{equation}
				\begin{aligned}
					L^{\star} \bvec{z}_{s} \stackrel{\text{sample}}{\sim} \mathcal{N}(\bvec{0}, \Sigma^{\star}) \qquad \forall s \in I_{n_{S}}
				\end{aligned}
			\label{Equation:CovarianceInclusion}
			\end{equation}
			
			To incorporate the expectance $\vec{\upmu}^{\star}$, simply add it to the transformed samples \eqref{Equation:ExpectanceInclusion}.
			
			\begin{equation}
				\begin{aligned}
					\vec{\upmu}^{\star} + L^{\star} \bvec{z}_{s} \stackrel{\text{sample}}{\sim} \mathcal{N}(\vec{\upmu}^{\star}, \Sigma^{\star}) \qquad \forall s \in I_{n_{S}}
				\end{aligned}
			\label{Equation:ExpectanceInclusion}
			\end{equation}
			
			Therefore, to sample from any given multivariate Gaussian distribution $\mathcal{N}(\vec{\upmu}^{\star}, \Sigma^{\star})$, simply draw iid samples through \eqref{Equation:iidGaussianSampling} and transform the samples through \eqref{Equation:ExpectanceInclusion}, and set ${^{s}}\bvec{f}^{\star} := \vec{\upmu}^{\star} + L^{\star} \bvec{z}_{s} \quad \forall s \in I_{n_{S}}$. Generating iid samples from standard normal distributions is much faster, especially with sophisticated libraries which can have pseudo-random number generators specifically for values drawn from the standard normal distribution.
			
			Moreover, to vectorise the computation for further computational efficiency, multiple samples can be obtained at once through \eqref{Equation:VectorisedSampling}.
			
			\begin{equation}
				\begin{aligned}
					\begin{bmatrix} {^{1}}\bvec{f}^{\star} & {^{2}}\bvec{f}^{\star} & \dots & {^{n_{S}}}\bvec{f}^{\star} \end{bmatrix} &= \begin{bmatrix} \vec{\upmu}^{\star} + L^{\star} \bvec{z}_{1} & \vec{\upmu}^{\star} + L^{\star} \bvec{z}_{2} & \dots & \vec{\upmu}^{\star} + L^{\star} \bvec{z}_{n_{S}} \end{bmatrix} \\
					&= \mathcal{U} + L^{\star} Z \qquad \text{where} \qquad \mathcal{U} := \{\vec{\upmu}^{\star}\}^{T}_{s \in I_{n_{S}}}
				\end{aligned}
			\label{Equation:VectorisedSampling}
			\end{equation}			
			
			Under usual circumstances, this transformation technique provides a more time efficient method for drawing samples from a multivariate Gaussian distribution. However, if the samples are to be used for Monte Carlo estimation of a certain quantity, such as the joint PIE, and such a quantity is involved in an optimisation process, the above technique is almost necessary \footnote{There are certainly optimisation algorithms that are designed specifically for such an optimisation process, and are generally found under the technique of \textit{stochastic optimisation}. The transformation technique here however avoids the need for such stochastic methods such that any regular optimiser can be employed.}. In fact, this is precisely the case for MCPIE acquisition, where the joint PIE is estimated through Monte Carlo sampling and is also the objective of the optimisation process (section \ref{InformativeSeafloorExploration:MCPIE}).
			
			The reason is essentially as follows. Since the MCPIE is estimated from samples that are randomly drawn from a probabilistic distribution, the MCPIE itself is random, even if its variance decreases under the limit of large samples. As such, for the same set of query locations, the optimiser may compute slightly different values for the corresponding MCPIE. This makes the optimisation process numerically unstable with poor convergence properties. In fact, under small samples where the variance of MCPIE is large, the optimisation process generally does not converge. This is because variance in the objective or constraint transfers to a variance in the optimiser Lagrangian, which acts as a noise input to the optimiser. Furthermore, most optimisers utilises numerical gradient methods, and derivatives amplifies noise \footnote{With minimum rigour, derivatives are limits of standardised differences whereas integrals are limits of standardised sums. Differences of noisy quantities are relatively more noisy, and sums of noisy quantities are relatively less noisy. As such, derivatives amplifies noise, while integrals smooths away noise. See \cite{Nise:2000:CSE:519085} for more detail.}. As such, the randomness spreads noise throughout the optimiser Lagrangian and corresponding gradients, which destabilises the process as, figuratively speaking, the optimiser is unable discern the peak of the mountain if the terrain is changing like the waves of an ocean. 
			
			The typical way to solve the above problem is to reinitialise the seed to the same value in each optimisation iteration, so that the same samples are drawn for estimation under identical scenarios. Nevertheless, if the transformation technique is used, this is equivalent to drawing a set of iid normal samples $Z$ at the beginning of the optimisation process and continuously reusing these samples for any computation. This avoids resetting the seed and redrawing the samples, thus speeding up the computation. This technique works as, while the distribution to be drawn from in each optimisation iteration changes, such a distribution is entirely captured by the mean vector $\vec{\upmu}^{\star}$ and covariance matrix $\Sigma^{\star}$. Thus, the only quantities that need to change in each iteration are $\vec{\upmu}^{\star}$ and $\Sigma^{\star}$, or equivalently $\mathcal{U}$ and $L^{\star}$ in \eqref{Equation:VectorisedSampling}. That is, through transforming the iid samples with dynamic moments $\vec{\upmu}^{\star}$ and $\Sigma^{\star}$ in every optimisation loop, the optimisation process stabilises and is more efficient than the method of seed reinitialisation.
			
	\section{Time Complexity}
	\label{Appendix:ComputationalAspects:TimeComplexity}
	
		Reducing computational time
		
%		\subsection{Numpy and Vectorisation}
		
		\subsection{Cholesky Update and Downdate}
		\label{Appendix:ComputationalAspects:TimeComplexity:CholeskyUpdateDowndate}
		
		\subsection{Caching learned GPs for fast prediction}
		
		\subsection{Parallelisation of GP learning and hyper-parameter batching}
		
		\subsection{Parallelisation of GP prediction and relevant subtleties}
		
%		\subsection{Fast vectorised GP drawing for regression and classification}
		
		
	\section{Time \& Spatial Complexity}
	\label{Appendix:ComputationalAspects:TimeSpaceComplexity}
	
		\subsection{Avoiding full covariance computations}
		\label{Appendix:ComputationalAspects:TimeSpaceComplexity:CovarianceAvoidance}
		
		\subsection{Taking advantage of diagonal log-likelihood Hessians}
		\label{Appendix:ComputationalAspects:TimeSpaceComplexity:DiagonalHessians}
		
		\subsection{Creating predictor objects to modularise prediction}
		
		